### 简介：

基于大量普通机器构建的分布式文件存储系统，特别适合大文件(>100MB)的批量顺序读写，随机少量数据的查询性能较差，在google内部应用场景有两种：一是程序员分析处理数据，二是线上数据的批量处理

### 接口：

create, write, append, delete, read

### 架构：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/2_storage/imgs/gfs_structure.png" width="800px"/>

整个系统包含三个主要模块**Master**, **ChunkServer**, **Client**。其中数据存储的实体为chunk，每个chunk都是普通linux系统上的一个大小固定为64MB的普通文件。

Master主要功能是**索引**：告诉客户端它要找的文件存储在哪个ChunkServer的chunk上。Master上存储了文件到具体Chunk位置的影射，新数据写入到哪个chunk上也有master来决定，同时它还负责垃圾回收，垃圾回收的流程比较简单，就是定期比对master上存储的索引数据与硬盘上的实际chunk，如果硬盘上的chunk在master上不存在与之对应的元数据，则视为垃圾数据，同时还会检查chunk上的checksum，不完整的数据也会被当作垃圾数据清理掉。除此之外，Master还负责维护replica的数量：如果某个chunkserver掉线，导致备份数量少了，Master会在其他地方新建chunk来补足；如果管理员更新了配置中replica的数量，Master也会做对应的增减。同时Master还会定期平衡数据，使各个磁盘的使用量尽量大致相同。以上数据移动，均由目标chunkserver主要拉取，而不是源chunkserver主动发送。

ChunkServer具体负责chunk数据上的读写。Client一旦通过Master确定了要读取的chunk的位置就不再与Master打交道，接下来所有的数据流都只走ChunkServer。ChunkServer上存储了Chunk的Checksum与version信息，可以在数据读取的时候进行校验，自动过滤调不完整或过期了的数据。ChunkServer会将自己chunk的位置存储起来(Master不会永久存储)，Master重启后需要由ChunkServer来向其通报chunk的位置。

Client也由GFS提供，其中包含比较重要的两部分信息，一是来自Master的chunk位置的缓存，二是针对chunk的version，前者可以减少与Master的交互次数，加快文件I/O速度，同时减轻Master的负担，后者version来自Master，可以防止读到过期数据。

该图还给出了**数据读取流程：**Master中存储了文件与chunk的影射关系，比如图中`/foo/bar`文件对应的chunk列表就是Master框图中右侧列表，第一个chunk的位置`2ef0`，首先客户端将文件名和chunk偏移量(chunk大小固定，偏移量可以算出来)发送给Master，Master据此返回chunk handle与chunk位置（论文中没有介绍chunk handle，个人猜测应该是类似与文件句柄的对象），客户端拿到chunk handle之后，并确定要读取的数据范围(byte range)，然后与chunkserver建立链接并读取数据。

### 数据写入流程：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/2_storage/imgs/gfs_write.jpg"/>

(1)client向Master询问primary节点(拥有Master赋予的lease)的位置；(2)Master告诉client primary以及其他replica的位置，这些信息会被缓存到客户端；(3)数据开始沿着精心挑选的管道传输到各个数据节点上(不一定从哪个节点开始)；(4)数据全部推送到各个节点上以后，client向primary发送数据写入请求，primary负责对并发的写入请求排序(consecutive serial number)；(5)primary将写入顺序发送其他replica上，并告诉它们按照这个顺序把数据写入chunk里；(6)数据写完后通知primary；(7)数据写入所有节点后，primary通知client数据写入成功。

### 数据一致性问题

数据不一致包含三种情况：写入失败导致的数据不完整(包含垃圾数据)、失败重写导致chunk中包含重复数据、chunkserver掉线导致的数据过期(stale replication)

客户端在写入时首先会把要写入的数据沿着一条精心挑选的网络管道传送到各个chunkserver上(不是并行的传到A，B，C上，而是沿着类似A->B->C这样的串行顺序传送)，在此期间拿到lease的chunkserver作为primary节点负责确定并发写入的顺序，然后告诉其他replica都按这个顺序将暂缓在chunserver中的数据写入到chunk中，写入成功后的各个replica处于一致但不确定的状态。在这个过程中如果有一个chunk写入失败，primary会告诉客户端，数据写入失败，让客户端重新发起写入请求，但在之前写入的数据不会回滚撤销(会变为垃圾数据等待清理)，此时各个replica处于不一致的状态，但即使如此客户端也不会读到不一致的数据(不同客户端从不同的备份节点上读取并返回给用户的数据肯定是一致的)，同时因为有checksum机制的存在，写入失败所产生的垃圾数据不会被返回给客户端。


append采用的是append-at-least-once机制，这样就有可能导致同一条数据被写入多次，此时需要由客户端通过数据的唯一标识来过滤。

如果某个chunkserver掉线了一段时间之后由重启了，而在此期间与之相关的文件数据已经发生了变化，而它所管理的chunk由于没有接收到更新而导致所包含的数据过期了，为了解决这个问题，Master再每次分配lease的时候都会更新一下版本号(该版本号同时也会发给客户端)，这样可以通过比较版本号来过滤过期数据。


### High Availability：

GFS假定所依赖的机器都是不可靠的，随时有可能因为各种原因掉线(关机、断网、故障等等)，为此GFS提供了一系列的机制来提高可用性：

(1) 快恢复：所有节点(master/chunkserver)都要能够在秒级重启，如果客户端连接某个节点失败，可以稍后重试(有可能导致服务短暂不可用)

(2) chunk备份：同一份数据的不同chunk尽量分布在不同的机架上，这样不但防止一损具损还能实现网络的负载均衡

(3) 通过影子Master来提高Master的可用性(并非一直同步，影子Master可能会稍稍落后于主Master)

### 使用情况

**测试环境：** 一个Master，两个影子Master，16个chunkserver，16个客户端，单台机器配置：双核1.4GHz PIII处理器、2GB内存、2个80GB的5400rpm的硬盘，以太网100Mbps，包含两个交换机，交换机之间的带宽是1Gpbs。
**读取速度：** 只有一个客户端的时候，速度能达到10MB/s，16个客户端同时读取的时候，单个客户端速度约为6MB/s
**写入速度：** 只有一个客户端的时候，速度为6.3MB/s，16个客户端同时写入的时候，单个客户端速度约为2.2MB/s
**append速度：** 只有一个客户端的时候，速度为6MB/s，16个客户端同时append的时候，单个客户端速度约为0.3MB/s

**google内部的典型使用情况**

| Cluster | A | B |
| ------ | ------ | ------ |
| Chunkservers | 342 | 227 |
| disk space | 72 TB | 180 TB |
| Used disk space | 55 TB | 155 TB |
| Number of Files | 735 k  | 737 k |
| Number of Dead files | 22 k | 232 k |
| Number of Chunks | 992 k | 1550 k |
| Metadata at chunkservers | 13 GB | 21 GB |
| Metadata at master | 48 MB | 60 MB |

