### 简介：

基于大量普通机器构建的分布式文件存储系统，特别适合大文件(>100MB)的批量顺序读写，随机少量数据的查询性能较差，在google内部应用场景有两种：一是程序员分析处理数据，二是线上数据的批量处理

### 接口：

create, write, append, delete, read

### 架构：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/2_storage/imgs/gfs.png" width="800px"/>

整个系统包含三个主要模块**Master**, **ChunkServer**, **Client**。其中数据存储的实体为chunk，每个chunk都是普通linux系统上的一个大小固定为64MB的普通文件。

Master主要功能是**索引**：告诉客户端它要找的文件存储在哪个ChunkServer的chunk上。Master上存储了文件到具体Chunk位置的影射，新数据写入到哪个chunk上也有master来决定，同时它还负责垃圾回收，垃圾回收的流程比较简单，就是定期比对master上存储的索引数据与硬盘上的实际chunk，如果硬盘上的chunk在master上不存在与之对应的元数据，则视为垃圾数据，同时还会检查chunk上的checksum，不完整的数据也会被当作垃圾数据清理掉。除此之外，Master还负责维护replica的数量：如果某个chunkserver掉线，导致备份数量少了，Master会在其他地方新建chunk来补足；如果管理员更新了配置中replica的数量，Master也会做对应的增减。同时Master还会定期平衡数据，使各个磁盘的使用量尽量大致相同。以上数据移动，均由目标chunkserver主要拉取，而不是源chunkserver主动发送。

ChunkServer具体负责chunk数据上的读写。Client一旦通过Master确定了要读取的chunk的位置就不再与Master打交道，接下来所有的数据流都只走ChunkServer。ChunkServer上存储了Chunk的Checksum与version信息，可以在数据读取的时候进行校验，自动过滤调不完整或过期了的数据。ChunkServer会将自己chunk的位置存储起来(Master不会永久存储)，Master重启后需要由ChunkServer来向其通报chunk的位置。

Client也由GFS提供，其中包含比较重要的两部分信息，一是来自Master的chunk位置的缓存，二是针对chunk的version，前者可以减少与Master的交互次数，加快文件I/O速度，同时减轻Master的负担，后者version来自Master，可以防止读到过期数据。


### 数据写入流程：


### 数据一致性问题

数据不一致包含三种情况：写入失败导致的数据不完整(包含垃圾数据)、失败重写导致chunk中包含重复数据、chunkserver掉线导致的数据过期(stale replication)

客户端在写入时首先会把要写入的数据沿着一条精心挑选的网络管道传送到各个chunkserver上(不是并行的传到A，B，C上，而是沿着类似A->B->C这样的串行顺序传送)，在此期间拿到lease的chunkserver作为primary节点负责确定并发写入的顺序，然后告诉其他replica都按这个顺序将暂缓在chunserver中的数据写入到chunk中，写入成功后的各个replica处于一致但不确定的状态。在这个过程中如果有一个chunk写入失败，primary会告诉客户端，数据写入失败，让客户端重新发起写入请求，但在之前写入的数据不会回滚撤销(会变为垃圾数据等待清理)，此时各个replica处于不一致的状态，但即使如此客户端也不会读到不一致的数据(不同客户端从不同的备份节点上读取并返回给用户的数据肯定是一致的)，同时因为有checksum机制的存在，写入失败所产生的垃圾数据不会被返回给客户端。


append采用的是append-at-least-once机制，这样就有可能导致同一条数据被写入多次，此时需要由客户端通过数据的唯一标识来过滤。

如果某个chunkserver掉线了一段时间之后由重启了，而在此期间与之相关的文件数据已经发生了变化，而它所管理的chunk由于没有接收到更新而导致所包含的数据过期了，为了解决这个问题，Master再每次分配lease的时候都会更新一下版本号(该版本号同时也会发给客户端)，这样可以通过比较版本号来过滤过期数据。


### Failover：


### 性能