### 简介
传统的分布式存储系统(如GFS)为了保证可用性，会把同一份数据在不同的地方存放多份，这样不但拖慢了数据写入速度，而且也浪费了很多网络带宽，针对这个问题本文通过lineage技术实现了一个基于内存的分布式数据共享系统：**Tachyon**(现已改名为`Alluxio`)。当出现数据丢失时，lineage可以基于之前所做的checkpoint进行回放(`recomputation`)来恢复数据，这一过程不需要其他节点上有备份数据，因此加快了数据恢复速度，而且节省了很多计算资源。

在作者看来，**应用lineage技术是将计算集群的I/O速度提升到内存水平的唯一途径**。

应用tachyon需要满足以下几个前提条件：
- 数据一旦就不再修改
- 任务是确定的(deterministic)
- 基于数据的存放位置进行调度
- 当前任务所处理的数据能够全部加载到内存(不是全部数据，是当前处理所涉及到的数据)
- 任务程序不能太大：程序可能需要在不同的节点上反复运行，程序迁移代价不能大于数据的迁移代价


### 工作流程说明
<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/2_storage/imgs/tachyon_flow.png"/>

假定任务`P`的输入文件集为`A`，输出文件集为`B`，P在把结果输出到B之前，需要先把lineage信息提交到tachyon里，这些信息描述如何运行`P`来由`A`产生`B`(比如命令行参数、配置信息等)。在这里能够回放(recomputation)的前提是输入数据(文件集`A`)不能变。

从以上流程可以看出，tachyon需要捕获并理解用户提交的任务`P`：tachyon内置了通用了的lineage API(见下表)可以捕获很多流行的分布式并行处理的框架(如mapreduce)，同时也为Spark/Hadoop提供了接口。

| Return | Signature |
| ------ | ------ |
| Global Unique Lineage Id | createDependency(inputFiles, outputFiles, binaryPrograms, executionConfiguration, dependencyType) |
| Dependency Info | getDependency(lineageId) |

`createDependency`中的参数`binaryPrograms`即为上文提到的任务程序`P`，这个程序需要由计算框架来写一个wrapper作为接口，使其既能理解框架里计算代码，也能理解tachyon的行为，tachyon也不会去解析程序的配置信息`executionConfiguration`，在tachyon看来这些配置就是普通的字节数组，具体的解析工作由wrapper来实现。

在实现这一系统时，遇到了两个挑战：
- 如何在长时间运行的系统上限制recomputation的时长(bounding the recomputation cost)
- 如何为recomputation分配资源？(如何在尽量不对其他任务产生影响的情况下尽快的执行recomputation)


### Bounding the recomputation cost（Edge Algorithm）
lineage本质上还是一个基于checkpoint做数据备份与恢复的技术，这里的关键点是何时做checkpoint，针对哪些文件做checkpoint？
- 在一个长时间运行的系统上，如果checkpoint的时机选择不好，lineage的链会很长，recomputation的时间也会很长
- 有些文件的访问频率要远高于其他文件，这类文件(热点文件)也应该被checkpoint
- 尽量避免checkpoint临时文件：Facebook给出的数据表明，超过70%的数据会在一天之内被删除，这类文件都不应该被checkpoint

基于以上考虑本文提出了Edge算法：将工作流中各个计算任务的输入/输出文件抽象程DAG(有向无环图)，其顶点是文件集，边表示依赖关系：如果文件集A是某个任务的输入，文件集B是其输出，则有一条由A指向B的有向边将二者连接起来。为了限制recomputation的时间，Edge算法会checkpoint这个DAG的所有边缘节点，除此之外，Edge算法还会checkpoint DAG内部的热点文件(访问次数超过2次的文件)。

Tachyon是一个基于内存的存储系统，大部分的I/O都在内存，tachyon采用上述Edge算法`异步`地将内存中的数据checkpoint到硬盘上。


### Resource allocation for recomputation
资源分配所需要考虑的问题包括：
- 支持优先级：recomputation的优先级应该与原任务相当，如果某个低优先级请求的数据丢了，需要回放，那么这个回放任务不应该对高优先级的任务产生影响，但后来这个文件又被某个高优先级的任务请求了，那么这个回放任务的优先级应该提高
- 资源动态分配：不应该为recomputation预留资源，需要recomputation的时候才为其分配资源
- 避免级联recomputation：针对这种A-->B-->C-->A带有循环依赖的情况，应该避免无穷递归recomputation

基于以上考虑，tachyon给出两种资源分配策略：
- 基于优先级的调度分配策略

为了避免对其他任务差生影响，默认情况下，所有recomputation任务的优先级都是最低的，但这样可能会产生优先级倒置的情况，为了解决这个问题，recomputation的优先级自动继承发起请求的任务的优先级

- 基于权重的调度分配策略

刚开始的时候，所有的recomputation都拥有最低权重(默认为1)，当某个任务对某个丢失的文件发起请求后，会将自己的一部分权重分配给对应的recomputation任务，由于recomputation的权重来自发起的任务，所以对其他任务没有影响。


### 系统架构
<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/2_storage/imgs/tachyon_architecture.png"/>

整个系统主要包含master, worker, client三个部分，其中master负责管理全部的元数据(目录结构、数据块的位置、worker节点的容量等)、监控各个worker节点的状态；worker负责本地资源以及文件数据块的管理，如接收/响应客户端的读写请求等；client是用户tachyon交互的接口，提供了很多针对java的文件操作接口，同时也支持REST, go以及python等。


### 性能
单看输入写入速度，tachyon是内存版的HDFS的110倍，在实际使用例子中，是内存版的HDFS的4倍，在需要故障恢复(大约需要1分钟左右)的情况下，依然是HDFS的3.8倍，HDFS社区已经开始学习tachyon采用lineage技术进行改造优化。

异步checkpoint的Edge算法优于任何基于固定时间间隔的算法。

数据回放(recomputation)最多消耗系统30%的资源，针对Facebook/Bing的调研发现，资源消耗分别是0.9%与1.6%，可以说是微乎其微。

相对于普通的复制方式，基于lineage的tachyon能节省接近一半的带宽，令T为任务做checkpoint的时间与任务执行时间的比值，该值越大，则节省的带宽越多:40%(T=4)--->50%(T>10)。
