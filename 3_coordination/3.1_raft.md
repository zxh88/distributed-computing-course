### 简介
在分布式系统中，为了容错，需要将同一个服务部署多台相互对立的机器上，但同时要保证相同的请求即使落在了不同的机器上，所得到的结果应该也是一样的，这就要求这些提供相同服务的机器所看到的环境(比如元数据)必须是一样的，由此引出了RSM(Replicated State Machine，复制状态机)的概念，这些环境就可以由RSM来抽象描述。

RSM最早是在图灵奖得主Leslie Lamport的著名论文"Time, clocks, and the ordering of events in a distributed system(1978)"论文中提出的，比较系统性的阐述是在Fred Schneider的论文"Implementing fault-tolerant services using the state machine approach(1990)"中。

它的基本思想是一个分布式的RSM系统由很多个replica组成，每个replica是一个状态机，它的状态保存在一组状态变量中。状态机的状态通过并且只能通过外部命令（commands)来改变。比如你可以把MySQL服务器想像成一个状态机。它每接收到一条带修改功能的SQL语句（比如update/insert)就会改变它的状态。一组配置好replication的MySQL servers就是典型的RSM。

RSM能够工作基于这样的假设：如果一些状态机具有相同的初始状态，并且他们接收到的命令也相同，处理这些命令的顺序也相同，那么它们处理完这些命令后的状态也应该相同。 因为replica都具有相同的状态，所以坏掉任何一个也没有关系，有了RSM之后理论上可以做到永远不会因为机器的物理故障而丢失数据。

RSM的结构如下图所示：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_rsm.png"/>

从图中可以看出，RSM是基于日志复制(replicate)实现一致性的，每个RSM都包含3个模块：Consensus Module, Log, State Machine。其中Consensus Module负责保证不同的RSM保持一致；Log缓存了客户端提交过来的日志项，每个日志项都包含一条命令；State Machine代表了当前RSM的状态，一致性算法就是要保证不同节点上的State Machine都是一样的。

需要注意的是，Log相当于数组(实际上Raft就是把Log当作数组看待的)，每个日志项都有自己的index(位置，表示到达时间的先后)，但State Machine就是一个类似于存储的东西，没有index这种概念，它存储的就是Log中的命令执行后的结果。下文所介绍的Raft算法会同时介绍Log的一致性与State Machine的一致性：Log的一致性比较的就是相同index上的数据是否一样，State Machine的一致性指的是在任何时刻有没有可能出现不同的内容。

在过去的几十年里，Paxos算法一直是一致性问题的主要解决方案，甚至成了一致性算法的代名词，该算法已经从理论上证明了是正确的，但理解起来非常困难，以至于很少有人能够正确的将其应用到实际系统中，基于该算法实现的ZooKeeper在公开的实现细节上也与Paxos算法有了较大的偏差。

为此，本文提出了一种将易于理解作为首要目标的一致性算法：**Raft**，该算法与Paxos的最大区别就是好理解，不过性能也不差，为了实现这个目标，Raft将整个一致性问题分解成了3个相对独立的子问题：
- 如何选举leader？
- 如何复制(replicate)日志？
- 如何保证安全？(safety, 确保不同机器上状态机以相同的顺序应用相同的命令)

### 集群架构
典型的Raft集群中包含5台机器，分为3个角色：`leader`, `follower`, `candidate`。

`leader`是整个集群唯一的对外接口，客户端读写都只经过leader，客户端提交过来的日志项(命令)也由leader并行的流向其他节点，刚开始的时候，客户端并不知道这个集群的leader是谁，它会随机的连接一台机器，如果这台机器不是leader，则将拒绝该连接，并告诉client谁才是leader。

follower可以看作是leader数据的备份节点，负责接收并存储leader发送过来的日志项，完全处于被动(passive)状态: 只监听不请求。

集群正常没出错的时候，所有节点的角色要么是leader，要么是follower，但若因leader掉线或网络故障等原因导致某个follower在election timeout内没有收到来自leader的心跳信息的话，该follower就会变为candidate，candidate会向其他节点拉票以期自己成为leader。

leader不是一成不变的，每个leader都有自己的任期，称为term，随着leader的变更，term单调递增。term的主要作用是标识数据版本或判断当前leader(小term值)是否已被其他节点(大term值)推翻(替代)。

下图是节点之间的状态转移图：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_status.png" width="500px"/>


基于Raft实现的集群可以保证以下几点：
- Election Safety: 任何时候集群中都只有一个合法的leader(可能会存在多个，但同一个term内只会有一个，且真正能对外提供服务的只有一个)
- Leader Append-Only: leader只会把客户端提交过来的日志添加日志后面，然后将这些日志复制到其他节点，在任何时候都不会删除或覆盖本节点之前的日志项(follower会)
- Log Matching：如果在不同节点上的日志中的某个位置(index)上对应的日志项的term一样，则其内容也一定是相同的(相同term意味着该日志项是被同一个leader复制过来的，一个leader不会在同一个位置发送两个不同的日志项)
- Leader Completeness：由之前某个term内的leader提交(commit)的日志项，在之后term的leader日志中一定还会存在(即使当前leader由于种种原因导致被推翻了，也不用担心自己已提交的日志会失效：它们依然会存在，不会被覆盖也不会被删除)
- State Machine Safety：如果某个节点已经在某个位置(index)上应用了一个日志项，其他节点绝不会在相同位置应用一个不同的日志项(这一点最根本也最重要，这就是数据一致性)

基于以上特性，Raft在本文一开始所强调的更强的一致性(enforce a stronger degree of coherency)可以概述为：
- 相同位置的日志项，如果term相同，则内容必定相同（term不同则视为不同的日志项，此时就视为出现了数据不一致），从另一个角度来讲，就某个指定的term而言，不同节点上**已有**的日志肯定是一致的(有的follower可能还没跟上leader，所以会缺一些日志项)，绝不会不同节点上相同term相同位置对应的日志项内容不一样；
- 状态机的内容时时刻刻都是一致的(不过因为各个节点是独立的将日志数据应用到状态机上的，所以时间上会有先后)；


### leader选举
刚开始的时候整个集群中并没有leader，所有节点初始角色都是follower，经过一段时间(election timeout，不同节点上时间长短不一样)后，率先过期的follower发现集群中没有leader，然后立马变为candidate开始拉票，希望自己成为leader，为了防止出现多个leader(`裂脑`现象)，不同节点上的过期时间是不一样的(随机)，变为candidate的节点，把当前term加1，并先为自己投一票，然后开始并行的向其他节点发送RequestVote RPC，携带的参数包括：
- term：我处于哪个任期
- candidateId：我是谁
- lastLogIndex：我log中最后一个日志项的index
- lastLogTerm：我log中最后一个日志项的term

接收到投票请求的节点(该节点处于哪种角色都有可能)，会首先比较term：如果自己的term比candidate的term新(大)，则拒绝为其投票，否则再看自己有没有已经为其他candidate投票了，如果投了的话则拒绝该次投票请求，如果以上条件都满足，最后比较日志：先看谁的更新(比较lastLogTerm)，一样新的话再比较谁的更全(比较lastLogTerm)。

candidate只要获得超过一半节点的投票就可以成功当选leader了，选举成功后，leader要做两件事：首先向集群中的所有其他节点发送一个心跳信息，宣告自己是leader了；然后提交一个no-op(不包含命令)的空日志项来获取当前最新的日志提交(提交的概念后面会讲)状态。

关于当前leader什么时候会因为什么原因被推翻，请看本文的第二幅图。

### 日志复制
日志复制的过程可以认为是数据写入到State Machine的过程。Raft中的日志复制机制非常简单：client将包含某条命令的日志项提交到leader，leader将其append到自己的log中，然后再将其复制(replicate)到其他节点(follower)上，只要集群中超过一半的节点收到了该日志项(且复制该日志项的leader没有被推翻，后面会将为什么)，就可以认为该日志项是可提交(commit)的了，可提交的意思是该日志项的index确定了，不会再发生变化了，而且该index上的日志项也不会被删除/覆盖/修改了，可以被节点放心的应用到State Machine上了。

日志复制的方法为AppendEntries RPC，携带的参数包括：
- term：leader的任期
- leaderId：leader的标识
- prevLogIndex：当前发送日志项的前一个日志的index
- prevLogTerm：prevLogIndex对应日志项的term
- entries[]：要发送的日志项的集合
- leaderCommit：leader已提交的日志项的最大index

在这个过程中可能出现的问题：

**(1)follower因为自己日志与leader上的不一致，而拒绝了这次日志复制**

follower看了一下，自己位于preLogIndex上的日志项的term不是preLogTerm，然后就立马拒绝了这次日志复制，并将preLogIndex位置上的日志项删除。leader收到拒绝消息后后退一个(preLogIndex-1，并)，并将preLogIndex对应的日志项添加到entries中重新发送，如果再被拒绝，继续重复以上过程。


**(2)follower因为自己的term比较新，而拒绝了这次日志复制**

这种情况说明集群中有新leader了，当前这个leader已经被推翻了，follower会拒绝这次复制，同时告诉leader最新的term是多少，当前leader接到反馈后更新term并自动转为follower，然后等待来自新leader的消息。

**(3)leader还没复制完(只有不到一半的节点收到了这个日志)自己就崩溃了**

此时客户端会因为等待超时而将这次提交视为失败，其他节点会因为election timeout超时而开始选举，这些未复制完的日志项既可能被删除也可能被保留：如果新当选的leader上没有这个日志项，就会被删除，如果新当选的leader上有这个日志项就会被保留(能不能成功应用到state machine上还要看新leader的造化)。


**(4)leader复制完了(超过一半的节点收到了这个日志)，但没来得及告诉客户端反馈就崩溃了**

此时客户端仍然会因为等待超时而将这次提交视为失败，这些日志项虽然已经复制到了大部分节点上，依然有可能被覆盖，主要原因是比较日志的时候先比较term后比较长短：新leader虽然日志短，但最后那个日志项新依然会把之前leader复制的日志项给覆盖掉，具体例子见论文图8。针对这种情况，raft给出的解决方案是：

**(5)leader正在复制的过程中，收到了一条心跳信息，得知自己被推翻了**

### 数据读取


### 成员变更