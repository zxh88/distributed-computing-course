### 简介
在分布式系统中，为了容错，需要将同一个服务部署多台相互对立的机器上，但同时要保证相同的请求即使落在了不同的机器上，所得到的结果应该也是一样的，这就要求这些提供相同服务的机器所看到的环境(比如元数据)必须是一样的，由此引出了RSM(Replicated State Machine，复制状态机)的概念，这些环境就可以由RSM来抽象描述。

RSM最早是在图灵奖得主Leslie Lamport的著名论文"Time, clocks, and the ordering of events in a distributed system(1978)"论文中提出的，比较系统性的阐述是在Fred Schneider的论文"Implementing fault-tolerant services using the state machine approach(1990)"中。

它的基本思想是一个分布式的RSM系统由很多个replica组成，每个replica是一个状态机，它的状态保存在一组状态变量中。状态机的状态通过并且只能通过外部命令（commands)来改变。比如你可以把MySQL服务器想像成一个状态机。它每接收到一条带修改功能的SQL语句（比如update/insert)就会改变它的状态。一组配置好replication的MySQL servers就是典型的RSM。

RSM能够工作基于这样的假设：如果一些状态机具有相同的初始状态，并且他们接收到的命令也相同，处理这些命令的顺序也相同，那么它们处理完这些命令后的状态也应该相同。 因为replica都具有相同的状态，所以坏掉任何一个也没有关系，有了RSM之后理论上可以做到永远不会因为机器的物理故障而丢失数据。

RSM的结构如下图所示：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_rsm.png"/>

从图中可以看出，RSM是基于日志复制(replicate)实现一致性的，每个RSM都包含3个模块：Consensus Module, Log, State Machine。其中Consensus Module负责保证不同的RSM保持一致；Log缓存了客户端提交过来的日志项，每个日志项都包含一条命令；State Machine代表了当前RSM的状态，一致性算法就是要保证不同节点上的State Machine都是一样的。

需要注意的是，Log相当于数组(实际上Raft就是把Log当作数组看待的)，每个日志项都有自己的index(位置，表示到达时间的先后)，但State Machine就是一个类似于存储的东西，没有index这种概念，它存储的就是Log中的命令执行后的结果。下文所介绍的Raft算法会同时介绍Log的一致性与State Machine的一致性：Log的一致性比较的就是相同index上的数据是否一样，State Machine的一致性指的是在任何时刻有没有可能出现不同的内容。

在过去的几十年里，Paxos算法一直是一致性问题的主要解决方案，甚至成了一致性算法的代名词，该算法已经从理论上证明了是正确的，但理解起来非常困难，以至于很少有人能够正确的将其应用到实际系统中，基于该算法实现的ZooKeeper在公开的实现细节上也与Paxos算法有了较大的偏差。


为此，本文提出了一种将易于理解作为首要目标的一致性算法：**Raft**，该算法与Paxos的最大区别就是好理解，不过性能也不差，为了实现这个目标，Raft将整个一致性问题分解成了3个相对独立的子问题：
- 如何选举leader？
- 如何复制(replicate)日志？
- 如何保证安全？(safety, 确保不同机器上状态机以相同的顺序应用相同的命令)

### 集群架构
典型的Raft集群中包含5台机器，分为3个角色：`leader`, `follower`, `candidate`。

`leader`是整个集群唯一的对外接口，客户端读写都只经过leader，客户端提交过来的日志项(命令)也由leader并行的流向其他节点，刚开始的时候，客户端并不知道这个集群的leader是谁，它会随机的连接一台机器，如果这台机器不是leader，则将拒绝该连接，并告诉client谁才是leader。

follower可以看作是leader数据的备份节点，负责接收并存储leader发送过来的日志项，完全处于被动(passive)状态: 只监听不请求。

集群正常没出错的时候，所有节点的角色要么是leader，要么是follower，在没有日志需要复制的时候，leader定期向其他follower发送心跳，若因leader掉线或网络故障等原因导致某个follower在election timeout内没有收到来自leader的心跳信息的话，该follower就会变为candidate，candidate会向其他节点拉票以期自己成为leader。

leader不是一成不变的，每个leader都有自己的任期，称为term，随着leader的变更，term单调递增。term的主要作用是标识数据版本或判断当前leader(小term值)是否已被其他节点(大term值)推翻(替代)。

下图是节点之间的状态转移图：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_status.png" width="500px"/>


基于Raft实现的集群可以保证以下几点：
- **Election Safety**: 任何时候集群中都只有一个合法的leader(可能会存在多个，但同一个term内只会有一个，且真正能对外提供服务的只有一个)
- **Leader Append-Only**: leader只会把客户端提交过来的日志添加日志后面，然后将这些日志复制到其他节点，在任何时候都不会删除或覆盖本节点之前的日志项(follower会)
- **Log Matching：如果在不同节点上的日志中的某个位置(index)上对应的日志项的term一样，则其内容也一定是相同的(相同term意味着该日志项是被同一个leader复制过来的，一个leader不会在同一个位置发送两个不同的日志项)
- **Leader Completeness**：由之前某个term内的leader提交(commit)的日志项，在之后term的leader日志中一定还会存在(即使当前leader由于种种原因导致被推翻了，也不用担心自己已提交的日志会失效：它们依然会存在，不会被覆盖也不会被删除)
- **State Machine Safety**：如果某个节点已经在某个位置(index)上应用了一个日志项，其他节点绝不会在相同位置应用一个不同的日志项(这一点最根本也最重要，这就是数据一致性)

基于以上特性，Raft在本文一开始所强调的更强的一致性(enforce a stronger degree of coherency)可以概述为：
- 相同位置的日志项，如果term相同，则内容必定相同（term不同则视为不同的日志项，此时就视为出现了数据不一致），从另一个角度来讲，就某个指定的term而言，不同节点上**已有**的日志肯定是一致的(有的follower可能还没跟上leader，所以会缺一些日志项)，绝不会不同节点上相同term相同位置对应的日志项内容不一样；
- 状态机的内容时时刻刻都是一致的(不过因为各个节点是独立的将日志数据应用到状态机上的，所以时间上会有先后)；


### leader选举
刚开始的时候整个集群中并没有leader，所有节点初始角色都是follower，经过一段时间(election timeout，不同节点上时间长短不一样)后，率先过期的follower发现集群中没有leader，然后立马变为candidate开始拉票，希望自己成为leader，为了防止出现多个leader(`裂脑`现象)，不同节点上的过期时间是不一样的(随机)，变为candidate的节点，把当前term加1，并先为自己投一票，然后开始并行的向其他节点发送RequestVote RPC，携带的参数包括：
- term：我处于哪个任期
- candidateId：我是谁
- lastLogIndex：我log中最后一个日志项的index
- lastLogTerm：我log中最后一个日志项的term

接收到投票请求的节点(该节点处于哪种角色都有可能)，会首先比较term：如果自己的term比candidate的term新(大)，则拒绝为其投票，否则再看自己有没有已经为其他candidate投票了，如果投了的话则拒绝该次投票请求，如果以上条件都满足，最后比较日志：先看谁的更新(比较lastLogTerm)，一样新的话再比较谁的更全(比较lastLogTerm)。

candidate只要获得超过一半节点的投票就可以成功当选leader了，选举成功后，leader要做两件事：首先向集群中的所有其他节点发送一个心跳信息，宣告自己是leader了；然后提交一个no-op(不包含命令)的空日志项来获取当前最新的日志提交(提交的概念后面会讲)状态。

关于当前leader什么时候会因为什么原因被推翻，请看本文的第二幅图。

本文推荐的election timeout的随机选取范围是**150-300ms**。

### 日志复制
日志复制的过程可以认为是数据写入到State Machine的过程。Raft中的日志复制机制非常简单：client将包含某条命令的日志项提交到leader，leader将其append到自己的log中，然后再将其复制(replicate)到其他节点(follower)上，只要集群中超过一半的节点收到了该日志项(且复制该日志项的leader没有被推翻，后面会将为什么)，就可以认为该日志项是可提交(commit)的了，可提交的意思是该日志项的index确定了，不会再发生变化了，而且该index上的日志项也不会被删除/覆盖/修改了，可以被节点放心的应用到State Machine上了。

日志复制的方法为AppendEntries RPC，携带的参数包括：
- term：leader的任期
- leaderId：leader的标识
- prevLogIndex：当前发送日志项的前一个日志的index
- prevLogTerm：prevLogIndex对应日志项的term
- entries[]：要发送的日志项的集合
- leaderCommit：leader已提交的日志项的最大index

在这个过程中可能出现的问题：

**(1) follower因为自己日志与leader上的不一致，而拒绝了这次日志复制**

follower看了一下，自己位于preLogIndex上的日志项的term不是preLogTerm，然后就立马拒绝了这次日志复制，并将preLogIndex位置上的日志项删除。leader收到拒绝消息后后退一个(preLogIndex-1，并)，并将preLogIndex对应的日志项添加到entries中重新发送，如果再被拒绝，继续重复以上过程。

**(2) follower因为自己的term比较新，而拒绝了这次日志复制**

这种情况说明集群中有新leader了，当前这个leader已经被推翻了，follower会拒绝这次复制，同时告诉leader最新的term是多少，当前leader接到反馈后更新term并自动转为follower，然后等待来自新leader的消息。

**(3) leader还没复制完(只有不到一半的节点收到了这些日志)自己就崩溃了**

此时客户端会因为等待超时而将这次提交视为失败，其他节点会因为election timeout超时而开始选举，这些未复制完的日志项既可能被删除也可能被保留：如果新当选的leader上没有这个日志项，就会被删除，如果新当选的leader上有这个日志项就会被保留(能不能成功应用到state machine上还要看新leader的造化)。


**(4) leader复制完了(超过一半的节点收到了这些日志)，但没来得及告诉客户端反馈就崩溃了**

此时客户端仍然会因为等待超时而将这次提交视为失败，这些日志项虽然已经复制到了大部分节点上，依然有可能被覆盖，主要原因是比较日志谁更up-to-date的时候先比较term后比较长短：新leader虽然日志短，但最后那个日志项新依然会把之前leader复制的日志项给覆盖掉，如下图所示：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/fig8_2.jpg"/>

在T2时刻，S1当选leader，在本机index(2)上接收到了日志项(x)并把它复制到了S1,S2两个节点上，然后宕机了；在T3时刻，S5获得了S3,S4的投票当选为新的leader，在本机的index(2)上添加了另外一个日志项(y)后宕机了；在T4时刻，S1重新当选leader，接收了一个新的日志项，并把之前T2时刻接收到的日志项(x)继续复制到了S3上，此时因为日志项(x)已被大部分节点接收，所以可以提交(commit)并应用(apply)了，但是此时S1又宕机了；在T5时刻，S5因为最近一个日志项的term(3)比较大，因此可以获得S2,S3,S4的投票，由此重新当选为leader，并把自己在index(2)上的日志项(y)复制到了所有节点上。此时因为之前index(2)已被前任leader(S1)确定提交(commit)了日志项(x)，而现任leader(S5)又在这个位置上提交了日志项(y)，因此违背了论文中提到的**State Machine Safety：如果某个节点已在某个位置上确定了一个操作命令，则其他节点不能再在当前位置上应用不同的命令**。

假如初始时`a=1`，S1提交的日志项(x)为`a=a+1`，S5提交的日志项(y)为`a=a+2`，如果出现了以上情况，S1,S2,S3上a的最终值为4，但S4,S5上因为没有执行`a=a+1`所以它们上的a为3，这时候就出现了数据不一致。

针对这种情况，raft给出的解决方案是：新leader不能只通过已复制成功的份数来确定来自旧leader的日志项能不能提交，新leader只能通过计数来标记自己复制的日志能否提交，根据之前提到的`Log Matching`性质，只要当前leader接收的日志在集群可以提交了，则旧leader的日志也自然而然的变为可提交的了。(如果不采用这条方案，`Leader Completeness`性质就得不到保证了：之前leader提交的日志被之后的leader覆盖了)。所以正确的流程应该如下图所示：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/fig8_1.jpg"/>

在T4时刻，S1接收到当前term(4)的日志项的时候应该优先将其复制到大部分节点，而不是操作之前leader的日志，这样可以阻止S5当选leader，如果S1在当选为leader时没有新的日志到来，也不能提交前任leader尚未提交的日志，只能等当前term有日志项来的时候才能操作。


**(5) leader正在复制的过程中，收到了一条心跳信息，得知自己被推翻了**

这种情况等同于(3)，如果能被推翻，说明至少一半的节点给新leader投票了，所以说当前leader复制成功的份数肯定不会超过一半。

在后三种情况中，从客户端的角度来看都属于提交失败的情况，但实际上客户端以为不会生效的命令很有可能被新leader复制成功并被应用到了state machine中，客户端对此是不知情的，如果此时客户端重新提交的话，就相当于执行了两边相同的命令，对于幂等的行为还好，不幂等的行为就会出错了(比如客户端提交了一个给小王转1000RMB请求，结果出现了后三种情况，提示失败了，但实际上执行了，小王以为失败了，又提交了一次转账请求...)，针对这种情况Raft给出的解决方案是：为客户端提交的每条命令关联一个唯一的标识(或这递增的时间戳)，相同命令标识相同，这样的话state machine就知道某条命令是不是已被执行过了。

### 数据读取
读操作非常简单，因为读不会改变state machine，所以针对这种操作，leader不会记录日志，但需要防备客户端连到了一个过期leader上：客户端连接的leader与其他节点失去了联系，变成了孤家寡人，客户端连接到这样的leader上是不会报错的，但很可能读到旧数据。为了防止这种情况的发生，针对只读操作，leader在返回读取的数据之前，必须要与大部分节点成功交换心跳信息。

### 成员变更
Raft协议支持在不需要停止服务的前提下，更换/增加集群成员。假定旧集群的配置信息为`C_old`，两者的合集为`C_old_new`，新集群的配置为`C_new`。配置信息也以日志项的形式提交，节点在收到配置信息日志项后就立马以此为准，不等到该日志项被leader标为可提交(commit)才生效。为了防止在新旧集群交替的过程中出现裂脑现象，整个过程分为两个阶段：

**(1) 将新成员拉进来**：客户端将C_old_new配置信息提交(submit)到旧集群的leader中，并使其生效(可commit)，此时集群处于`joint consensus`状态，在此之后的复制/选举操作需要分别得到新/旧集群中的大部分节点的同意；

**(2) 把旧成员剔出去**：客户端将C_new配置信息提交(submit)到当前集群的leader中，并使其生效(可commit)，此时新旧交替完成，新的集群配置信息不再包含旧成员节点，选举/复制都不会涉及到旧成员节点，这时候旧的成员节点可以关机下线了。

为什么不直接提交`C_new`而要分两步？如果在提交`C_new`的时候失败了，比如说只有新节点和不超过一半的旧集群节点收到了`C_new`，此时新旧集群完全有可能产生两个leader。

在这个过程中还有3个问题需要注意：

- 旧集群已经积累了很多数据了，新集群加进来需要很久才能赶上来，此时可以限制新集群的节点始终为follower节点(不会超时造反)，等到数据复制的差不多了再给予选举权
- 当其集群的leader不在新的配置信息中，此时的leader应该只有管理权，数据复制时不能把自己算在里面
- 旧集群的成员节点在被剔除后没有及时下线，仍然在超时发送拉票请求，这种请求可能会干扰新集群的正常工作，此时可设置一个最小过期时间，在这个时间内接收到投票请求，统统忽略


### 论文图2翻译总结：

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_state.png"/>

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_append_entries.png"/>

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_request_vote.png"/>

<img src="https://github.com/zxhcodes/distributed-computing-course/blob/master/3_coordination/imgs/raft_rules.png"/>
